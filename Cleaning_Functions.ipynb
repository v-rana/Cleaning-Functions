{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duyW-fiwaOB_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string as st\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "porter = PorterStemmer()\n",
        "\n",
        "\n",
        "\n",
        "class cleaning_text:\n",
        "  def remove_punct(text):\n",
        "    return (\" \".join([ch for ch in text if ch not in st.punctuation]))\n",
        "\n",
        "  def tokenize(text):\n",
        "    text = re.split('\\s+' ,text)\n",
        "    return [x.lower() for x in text]\n",
        "  \n",
        "  def remove_small_words(text):\n",
        "    return [x for x in text if len(x) > 3 ]\n",
        "  \n",
        "  def remove_stopwords(text):\n",
        "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]\n",
        "  \n",
        "  def stemming(text):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(word) for word in text]\n",
        "  \n",
        "  def lemmatize(text):\n",
        "    word_net = WordNetLemmatizer()\n",
        "    return [word_net.lemmatize(word) for word in text]\n",
        "  \n",
        "  def return_sentences(tokens):\n",
        "    return \" \".join([word for word in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(col):\n",
        "  col = col.str.replace('[{}]'.format(st.punctuation), '')\n",
        "  col = col.apply(lambda x: cleaning_text.tokenize(x))\n",
        "  col = col.apply(lambda x: cleaning_text.remove_small_words(x))\n",
        "  col = col.apply(lambda x: cleaning_text.remove_stopwords(x))\n",
        "  #col = col.apply(lambda x: cleaning_text.stemming(x))\n",
        "  col = col.apply(lambda x: cleaning_text.lemmatize(x))\n",
        "  col= col.apply(lambda x: cleaning_text.return_sentences(x))\n",
        "  return col\n"
      ],
      "metadata": {
        "id": "uNzq2yw4aYjX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}